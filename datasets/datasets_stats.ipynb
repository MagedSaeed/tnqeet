{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f10f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82191181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "# Load both datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = load_dataset(\"MagedSaeed/tnqeet-training-datasets\", \"all_shuffled\")['train'] # type: ignore\n",
    "test_dataset = load_dataset(\"MagedSaeed/tnqeet-testing-datasets\", \"all_shuffled\")['test'] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd14fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Count words in a text string\"\"\"\n",
    "    return len(str(text).split())\n",
    "\n",
    "def get_dataset_table():\n",
    "    \"\"\"Create a table with dataset statistics by source\"\"\"\n",
    "    \n",
    "    # Convert to dataframes and add word counts\n",
    "    train_df = pd.DataFrame(train_dataset) # type: ignore\n",
    "    train_df['word_count'] = train_df['text'].apply(count_words)\n",
    "    \n",
    "    test_df = pd.DataFrame(test_dataset) # type: ignore\n",
    "    test_df['word_count'] = test_df['text'].apply(count_words)\n",
    "    \n",
    "    # Create table data\n",
    "    table_data = []\n",
    "    \n",
    "    # Add training dataset header\n",
    "    table_data.append([\"=== TRAINING DATASET ===\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "    \n",
    "    # Add training dataset sources\n",
    "    train_total_samples = 0\n",
    "    train_total_words = 0\n",
    "    for source in sorted(train_df['source'].unique()):\n",
    "        source_data = train_df[train_df['source'] == source]\n",
    "        domain, language = get_domain_and_language(source)\n",
    "        \n",
    "        samples = len(source_data)\n",
    "        total_words = source_data['word_count'].sum()\n",
    "        train_total_samples += samples\n",
    "        train_total_words += total_words\n",
    "        \n",
    "        table_data.append([\n",
    "            source,\n",
    "            domain,\n",
    "            language,\n",
    "            f\"{samples:,}\",\n",
    "            f\"{total_words:,}\",\n",
    "            f\"{source_data['word_count'].mean():.1f}\",\n",
    "            f\"{source_data['word_count'].max():,}\",\n",
    "            f\"{source_data['word_count'].min()}\"\n",
    "        ])\n",
    "    \n",
    "    # Add training dataset total row\n",
    "    table_data.append([\n",
    "        \"TOTAL\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        f\"{train_total_samples:,}\",\n",
    "        f\"{train_total_words:,}\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\"\n",
    "    ])\n",
    "    \n",
    "    # Add testing dataset header\n",
    "    table_data.append([\"=== TESTING DATASET ===\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "    \n",
    "    # Add testing dataset sources\n",
    "    test_total_samples = 0\n",
    "    test_total_words = 0\n",
    "    for source in sorted(test_df['source'].unique()):\n",
    "        source_data = test_df[test_df['source'] == source]\n",
    "        domain, language = get_domain_and_language(source)\n",
    "        \n",
    "        samples = len(source_data)\n",
    "        total_words = source_data['word_count'].sum()\n",
    "        test_total_samples += samples\n",
    "        test_total_words += total_words\n",
    "        \n",
    "        table_data.append([\n",
    "            source,\n",
    "            domain,\n",
    "            language,\n",
    "            f\"{samples:,}\",\n",
    "            f\"{total_words:,}\",\n",
    "            f\"{source_data['word_count'].mean():.1f}\",\n",
    "            f\"{source_data['word_count'].max():,}\",\n",
    "            f\"{source_data['word_count'].min()}\"\n",
    "        ])\n",
    "    \n",
    "    # Add testing dataset total row\n",
    "    table_data.append([\n",
    "        \"TOTAL\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        f\"{test_total_samples:,}\",\n",
    "        f\"{test_total_words:,}\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\"\n",
    "    ])\n",
    "    \n",
    "    # Create table\n",
    "    headers = [\n",
    "        \"Dataset Name\",\n",
    "        \"Domain\",\n",
    "        \"Language Type\",\n",
    "        \"Samples\",\n",
    "        \"Total Words\",\n",
    "        \"Avg Words\",\n",
    "        \"Max Words\",\n",
    "        \"Min Words\"\n",
    "    ]\n",
    "    \n",
    "    table = tabulate(table_data, headers=headers, tablefmt=\"grid\")\n",
    "    print(table)\n",
    "\n",
    "def get_domain_and_language(source):\n",
    "    \"\"\"Get domain description and language type for each source\"\"\"\n",
    "    source_info = {\n",
    "        'iwslt': ('Translation/Speech', 'MSA'),\n",
    "        'arabic_wikipedia': ('Wikipedia', 'MSA'),\n",
    "        'tashkeela': ('Religious/Classical', 'Classical Arabic'),\n",
    "        'annotated_aoc': ('Social Media', 'Dialectal'),\n",
    "        'oscar_small': ('Web Text', 'MSA/Dialectical'),\n",
    "        'ashaar': ('Poetry/Literature', 'Classical Arabic'),\n",
    "        'sanad': ('News/Media', 'MSA'),\n",
    "        'wasm': ('Social Media', 'Dialectal'),\n",
    "        'LLMs_abstracts': ('Academic', 'MSA'),\n",
    "        \"arabic_english_code_switching\":(\"Code Switching\",\"Dialectal\"),\n",
    "        \"arasum\":(\"News/Media\", \"MSA\"),\n",
    "        \"kind\":(\"Social Media\", \"Dialectal\"),\n",
    "        \"poetry\":(\"Poetry/Literature\", \"Classical Arabic\"),\n",
    "        \"quran\":(\"Religious/Classical\", \"Classical Arabic\"),\n",
    "        \"social_media\":(\"Social Media\", \"MSA/Dialectical\"),\n",
    "    }\n",
    "    return source_info.get(source, ('Unknown', 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea87c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| Dataset Name                  | Domain              | Language Type    | Samples   | Total Words   | Avg Words   | Max Words   | Min Words   |\n",
      "+===============================+=====================+==================+===========+===============+=============+=============+=============+\n",
      "| === TRAINING DATASET ===      |                     |                  |           |               |             |             |             |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| annotated_aoc                 | Social Media        | Dialectal        | 215,946   | 7,112,034     | 32.9        | 1,303       | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| arabic_wikipedia              | Wikipedia           | MSA              | 1,087,933 | 258,676,741   | 237.8       | 43,103      | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| ashaar                        | Poetry/Literature   | Classical Arabic | 250,229   | 35,602,521    | 142.3       | 55,252      | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| iwslt                         | Translation/Speech  | MSA              | 140,828   | 2,767,095     | 19.6        | 82          | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| oscar_small                   | Web Text            | MSA/Dialectical  | 407,864   | 18,960,383    | 46.5        | 20,820      | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| sanad                         | News/Media          | MSA              | 141,640   | 35,788,573    | 252.7       | 6,588       | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| tashkeela                     | Religious/Classical | Classical Arabic | 1,291,860 | 71,009,091    | 55.0        | 2,630       | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| wasm                          | Social Media        | Dialectal        | 52,320    | 1,054,926     | 20.2        | 65          | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| TOTAL                         |                     |                  | 3,588,620 | 430,971,364   |             |             |             |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| === TESTING DATASET ===       |                     |                  |           |               |             |             |             |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| LLMs_abstracts                | Academic            | MSA              | 500       | 41,111        | 82.2        | 205         | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| arabic_english_code_switching | Code Switching      | Dialectal        | 500       | 11,102        | 22.2        | 74          | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| arasum                        | News/Media          | MSA              | 500       | 185,471       | 370.9       | 2,145       | 57          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| iwslt                         | Translation/Speech  | MSA              | 500       | 9,566         | 19.1        | 65          | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| kind                          | Social Media        | Dialectal        | 500       | 7,824         | 15.6        | 124         | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| poetry                        | Poetry/Literature   | Classical Arabic | 500       | 69,297        | 138.6       | 1,995       | 14          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| quran                         | Religious/Classical | Classical Arabic | 500       | 9,093         | 18.2        | 75          | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| social_media                  | Social Media        | MSA/Dialectical  | 500       | 430,883       | 861.8       | 1,499       | 150         |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| tashkeela                     | Religious/Classical | Classical Arabic | 500       | 27,373        | 54.7        | 267         | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| wasm                          | Social Media        | Dialectal        | 500       | 9,781         | 19.6        | 59          | 10          |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n",
      "| TOTAL                         |                     |                  | 5,000     | 801,501       |             |             |             |\n",
      "+-------------------------------+---------------------+------------------+-----------+---------------+-------------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "get_dataset_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
